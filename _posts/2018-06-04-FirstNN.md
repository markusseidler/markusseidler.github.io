---
title: "My first Neural Network"
date: 2018-06-04
tags: [machine learning, neural network]
header:
    image: "images/FIRSTNN/starry_night.jpg"
excerpt: "My first Neural Network. Simple and easy."
---


## My first Neural Network

This is a huge and intimidating topic. It sounds complicated, mysterious,
and impossible to master if you do not have a PhD in Maths, Computer Science,
and Artifical Intelligence. But... it is incredibly exciting. I believe,
as long as you are excited about something you can master it. At the end,
90% of the problems in the world, need just one basic ingredient.
Common sense.

Thinking about Neural Network is for me pure excitement and the more I
explore it, the more I see the power of it. Imagine, how our world will
look like once machines can see, move, and think as individual units and
as part of our society. I am fully certain that this will come and exploring
Neural Networks with today's data availability and computer processing power
is a first step on a journey towards this future.

As the famous Chinese philosopher and founder of the Taoism Lao Zi (老子) said:

## "千里之行始于足下"

("A Journey of a thousand miles begins with a single step")

### The XOR problem

One of the reasons why Neural Networks are so powerful is that they are
able to solve non-linear problems. A famous non-linear problem in Machine
Learning is the XOR problem.

Mapping out all inputs and outputs would look like this:

<ul>
    <li>Input A: 0   Input B: 0   Output: 1</li>
    <li>Input A: 0   Input B: 1   Output: 0</li>
    <li>Input A: 1   Input B: 0   Output: 0</li>
    <li>Input A: 1   Input B: 1   Output: 1</li>
</ul>


Do you see the pattern?

The "exclusive or" problem is a function that has two inputs (A,B) and
which returns a true value (1) if both inputs are the same. Otherwise
the result is False (0). This problem looks very trival. However, the
complexity comes from the fact that it is non-linear. The four solutions
cannot be linearly separated. Linear classifiers and one-layer neural
network struggle to solve this problem.

## A Three-Layer-Network

Time for something new...

I created a simple fully-connected neural network. It conists of an input layer
and two hidden layers. How does it look like if we map it out? Do you really
wanna see it? I cannot really draw but I try my best.

<img src="{{ site.url }}{{ site.baseurl }}/images/FIRSTNN/ANN structure.jpg"
alt="ANN Three Layer Net">

Well, I told you that I cannot draw but I hope you still see it. First, it has
an input layer X with two features. We have for each observation two inputs (0,1).
After the input layer follows the first hidden layer. In my attempt to
draw it, I drew 3 neurons (or nodes) in the first hidden layer. This layer and
the network are fully connected. That means all neurons, input features or output
values are connected with each other from one layer to another.

The first neuron gets input from the input feature A of X multiplied by the strength
of the connection weight 1. It also gets input from feature B of X mutliplied
by weight 2. The result is Z2, a dot product of X with A and B features and the
first weight matrix W1.

Before the sum of its connections with the input features is passed on to
the next layer, to the second hidden layer, the neurons in the first layer process
the information. It does this with the help of an activation function. The activation
function is one of the secret ingredients of a neural network because it
translates linear relationships to non-linear activations. It changes Z2 to A2.

In order to achieve this, there are a number of commonly used activation functions.
A famous one which is often used for simple problems is the sigmoid function. The
sigmoid function takes a value and translates it to an output between 0 and 1.

This is the formula for the sigmoid function and its derivative:

<img src="{{ site.url }}{{ site.baseurl }}/images/FIRSTNN/sigmoid function_rs.jpg"
alt="Sigmoid function and derivatives">

The derivative of the sigmoid function can be written in two ways and both
have the same results. The difference is whether x is the input of the derivative or
whether the sigmoid function is used to calculate it. This understanding took
me one full day to figure it out... haha... but it's like this. In coding
scripts, you may see more often the f(x)*(1-f(x)) formula for the sigmoid derivative as it
is a bit more intuitive and easier to process.

We can easy plot the functions with python and the help of numpy.
This is the code:

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid (x):
    return (1/(1+np.exp(-x)))

def sigmoid_prime (x):
    return (np.exp(-x)/((1+np.exp(-x))**2))

X = np.linspace(-6,6,100)
y_sig = sigmoid(X)
y_sig_prim = sigmoid_prime(X)

plt.scatter (X,y_sig,linewidths=0.5, color="red", alpha=0.3, label="Sigmoid function")
plt.plot (X,y_sig_prim, linewidth=3, label="Derivatives of Sigmoid")
plt.legend()
plt.savefig("sigmoid.jpg")
```

I posted the graph below. The red line is the sigmoid function. The blue line
its derivative. The function graph looks like a S. Therefore, the name sigmoid which is
derived from the Greek letter "s".

We can see that the limit of the sigmoid function is 0 and 1
and both values are only reached in infinity. This means for the derivative
that very big and very small input values lead to a derivative close to 0.
On the other hand, the change of the sigmoid function, the derivative is the
largest in the middle.

<img src="{{ site.url }}{{ site.baseurl }}/images/FIRSTNN/sigmoid.jpg"
alt="Sigmoid function and its derivative">

The sigmoid function is just one example of an activation function. There
are many more such as TanH, Binary Step, and Rectified Linear Unit (ReLU).
For this example, sigmoid function is just fine.

## Ok, but how do you code this network?

The state-of-the-art libaries for deep learning and artifical neural networks
(ANN) are TensorFlow, Theano, Keras and PyTorch. These are all open-source
libraries which allow building neural networks in a few lines of code or
which gives you the flexiblity of building sophisticated and deep networks
with a less complicated python script.

However, in order to really control the architecture and to get the best
results, I think, it is crucial to understand the most important aspects
of neural networks. Therefore, I decided  to build  one from scratch without
the help of these libraries. I only used numpy for quick matrix and vector
calculations.

I applied object-orientated programming and built the ANN as a
python class. Furthermore, I split the basic ANN script from the
execution script. This allowed me wrapping the execution in loops without
modifying my original ANN script. I just needed to import my ANN script in
my execution file. I called the execution file "main.py" and the ANN script is
in a file called "ann.py".

Let me show you first the script for the basic neural network. This is ann.py

```python

import numpy as np

#Creating the Neural Network as Python Class
class ThreeLayerNet(object):

#Defining the relevant variables and external inputs.
    def __init__(self, X, y, neurons_l2=3, neurons_l3=3, weight_init="uniform", epoch=100):
        self.X = X
        self.y = y
        self.neurons_l2 = neurons_l2
        self.neurons_l3 = neurons_l3
        self.weight_init = weight_init
        self.epoch = epoch
        self.variable_list = ["self.X", "self.y",
                              "self.W1", "self.z2", "self.a2",
                              "self.W2", "self.z3", "self.a3",
                              "self.W3", "self.z4", "self.y_est",
                              "self.error", "self.cost", "self.cost_deriv",
                              "self.delta4", "self.dCdW3",
                              "self.delta3", "self.dCdW2",
                              "self.delta2", "self.dCdW1",
                              ]
        #Three ways to initiate the network weights.
        #Choice 1: Random with normal distributuon.
        if self.weight_init == "normal":
            self.W1 = np.random.randn(self.X.shape[1],self.neurons_l2)
            self.W2 = np.random.randn(self.neurons_l2, self.neurons_l3)
            self.W3 = np.random.randn(self.neurons_l3, self.y.shape[1])

        #Choice 2: Random with uniform distribution.
        elif self.weight_init == "uniform":
            self.W1 = np.random.uniform(0,1,self.X.shape[1]*self.neurons_l2).\
                reshape((self.X.shape[1],self.neurons_l2))
            self.W2 = np.random.uniform(0,1,self.neurons_l2*self.neurons_l3).\
                reshape((self.neurons_l2,self.neurons_l3))
            self.W3 = np.random.uniform(0,1,self.neurons_l3*self.y.shape[1]).\
                reshape((self.neurons_l3,self.y.shape[1]))

        #Choice 3: All weights are initially set at 1.
        else:
            self.W1 = np.ones((self.X.shape[1],self.neurons_l2))
            self.W2 = np.ones((self.neurons_l2,self.neurons_l3))
            self.W3 = np.ones((self.neurons_l3,self.y.shape[1]))

    #Sigmoid function
    def sigmoid (self, z):
        return (1/(1+np.exp(-z)))

    #Derivative of Sigmoid function.
    def sigmoid_deriv (self, z):
        return (np.exp(-z)/(np.power((1+np.exp(-z)),2)))

    '''
    This function manages the so-called forward propagation. Calculating the output
    step by step.

    The dot product of the first layer (input layer X) and weights matrix W1 results
    in the second layer (z2). Then applying the activation function sigmoid to z2
    and receiving the first activation layer a2. The next step is connecting a2 with
    the second hidden layer by calculating the dot product of a2 and weight matrix W2.
    This gives z3 and passing it through the sigmoid function leads to a3. Dot product
    of a3 with W3 is z4 and applying the sigmoid function on the last layer gives us
    the estimated output y_est.
    '''

    def forward_propagation(self):
        self.z2 = np.dot(self.X,self.W1)
        self.a2 = self.sigmoid(self.z2)

        self.z3 = np.dot(self.a2,self.W2)
        self.a3 = self.sigmoid(self.z3)

        self.z4 = np.dot(self.a3,self.W3)
        self.y_est = self.sigmoid(self.z4)

    '''
    This function calculates the error, the cost function, the difference between
    the estimated output y_est and the actual value y. It also calculates the
    derivative as a first input for backpropagation.
    '''

    def loss_calculation(self):
        self.error = self.y - self.y_est
        self.cost = 0.5*np.sum((np.power(self.error,2)))
        self.cost_deriv = self.y_est - self.y

    '''
    This is the famous backpropagation. The major technique that allows an ANN
    to learn and improve its results. Basically, what it does is it takes the prediction
    error and tries to see which parts of the network, which weights are most important
    in creating an error.

    Mathematically, we take the partial derivatives layer by layer
    and end up with the calculated sensitivity in respect to the weight matrices.

    Delta is the sensitivity of the layer cost and dCdW puts this sensitivity in respect
    to a specific weight matrix. In other words, how much does the error (or cost C)
    change if we change the weights W.
    '''

    def back_propagation(self):
        self.delta4 = np.multiply(self.cost_deriv,self.sigmoid_deriv(self.z4))
        self.dCdW3 = np.dot(self.a3.T,self.delta4)

        self.delta3 = np.multiply(np.dot(self.delta4,self.W3.T),self.sigmoid_deriv(self.z3))
        self.dCdW2 = np.dot(self.a2.T,self.delta3)

        self.delta2 = np.multiply(np.dot(self.delta3,self.W2.T),self.sigmoid_deriv(self.z2))
        self.dCdW1 = np.dot(self.X.T,self.delta2)

    '''
    This function trains the network by applying for each epoch the forward propagation,
    the loss_calculation and the back_propagation. After that it updates the weight
    by the sensitivities and recalculates everything for the next round, the next epoch.
    '''

    def train_NN (self):
        for i in range(0,self.epoch+1,1):
            self.forward_propagation()
            self.loss_calculation()
            self.back_propagation()
            self.W1 -= self.dCdW1
            self.W2 -= self.dCdW2
            self.W3 -= self.dCdW3

    #Printing the shape of all matrices
    def print_shape(self):
        for var in self.variable_list:
            print ("\n {} \n {}".format(var, eval(var).shape))

    #Printing the values of all matrices
    def print_values (self):
        for var in self.variable_list:
            print ("\n {} \n {}".format(var, eval(var)))

```

This includes quite a lot of concepts. So, let's break it down step by
step.

First, I created a python class and name it "ThreeLayerNet". By initiating
the class, I also defined the class relevant variables. I also put all
the variables in a list. That made it easier for me to iterate through
all the variables when I wanted to print values and shapes of each layer.

One point often raised by critical voices is that neural networks are a
"black box". For me it was important that I understand what happened at
each layer and in each neuron. The functions for printing the shape and values
are at the lower part of the script.

I also decided to see what happened if the weight initiation follows different
principles. Every ANN needs to start somewhere. At the very beginning, at the
time when the weights are not trained, the weights need to be set somewhere.
There are couple of ways to do that and research showed that weight initiation
is a critical part of the training success of neural networks. Networks can
be initiated with weights that are chosen randomly. I built in a function to choose
between random weights following the standard normal distribution (mean: 0,
std: 1), random weights with uniform distribution (all values have same probability),
and weights starting all at 1. The default option is "uniform". After that,
you can see the code for the sigmoid function and for its derivative.

## Back and forth

How does a neural network learn? Very simple. How do you learn? You practice,
you fail, you improve, you practice, you fail, you improve again. However, every time you fail, you (hopefully)
adjust your methodology, your strategy, your approach. Consequently, you get
better with experience.

For neural networks, the practice part is called "forward progatation". An observation, like
in our XOR case, the first observation, Input A: 0 and Input B: 0, is pushed
through the network and each layer calculates the value of its neuron activation. Each
neuron is connected with a set of weights.

At the end, we receive an estimated output. For example, the network suggests with
an input A of 0 and B of 0, the output is 0. This is wrong. The network failed.
The actual output is 1. In order to quantify the failure, we calculate the error
with a loss or cost function.

After a round of practice and after a big fail, the network improves with
the concept of backprogagation. It pushes the error backwards through the network
and tries to identify how important a weight is for an improvement. After that it
adjusts its weight and calculates again the output. The concept of pushing the error
backwards through the network is based on taking the partial derivatives for each layer.
Partial derivatives, layer by layer, can be quite efficiently calculated by applying
the so-called chain rule of derivatives. The result is the "gradient", a derivative
matrix or vector that shows how the output error is changing in respect to the weight
matrix W1, W2, and W3.

Basically, it is an optimization problem and the technique to find the optimal point is
something called "Gradient Descent". With the help of the various partial derivatives,
the network finds the slope of each variable that helps to bring down the error.
Some would describe it as a hiker who wants walk downhill from the peak to the lowest
point of the valley. The hiker takes small steps and continue its path always in the
direction of its steepest step downhill. Once the hiker is at the point where all
steps around him have a slop upwards, the hiker knows he or she has reached the lowest point.
In practice, the challenge is that the hiker would not know whether he or she has is
at a local minimum or global minimum, a low point or the absolute lowest point of the valley.
A solution to this is a technique called "Stochastic Gradient Descent" that chooses
randomly sub-samples of observations to calculate gradients.

Once I knew, the improvements I needed to apply to the three weight matrices, I
could change them and finish this round of calculations. Each round included the
forward propagation, loss calculation and backpropagation. One full round is called in
machine learning "one epoch". The function "train_NN" reiterated through one epoch and
adjusted the weights by the gradient at the end of each epoch before it used them for
the next epoch.

### Let's machine learn and fail?

Curious? Whether the neural network can learn the pattern of the XOR problem?
Do you think it will learn the pattern of this non-linear problem? Or fail? Let's try.

As mentioned before, for execution of the script, I created a second file called "main.py" and
leave the script with the neural network untouched.

This is main.py.






---
title: "My first Neural Network"
date: 2018-06-04
tags: [machine learning, neural network]
header:
    image: "images/FIRSTNN/starry_night.jpg"
excerpt: "My first Neural Network. Simple and easy."
---


## My first Neural Network

This is a huge and intimidating topic. It sounds complicated, mysterious,
and impossible to master if you do not have a PhD in Maths, Computer Science,
and Artifical Intelligence. But... it is incredibly exciting. I believe,
as long as you are excited about something you can master it. At the end,
90% of the problems in the world, need just one basic ingredient.
Common sense.

Thinking about Neural Network is for me pure excitement and the more I
explore it, the more I see the power of it. Imagine, how our world will
look like once machines can see, move, and think as individual units and
as part of our society. I am fully certain that this will come and exploring
Neural Networks with today's data availability and computer processing power
is a first step on a journey towards this future.

As the famous Chinese philosopher and founder of the Taoism Lao Zi (老子) said:

## "千里之行始于足下"

("A Journey of a thousand miles begins with a single step")

### The XOR problem

One of the reasons why Neural Networks are so powerful is that they are
able to solve non-linear problems. A famous non-linear problem in Machine
Learning is the XOR problem.

Mapping out all inputs and outputs would look like this:

<ul>
    <li>Input A: 0 Input B: 0 Output: 1</li>
    <li>Input A: 0 Input B: 1 Output: 0</li>
    <li>Input A: 1 Input B: 0 Output: 0</li>
    <li>Input A: 1 Input B: 1 Output: 1</li>
</ul>


Do you see the pattern?

The "exclusive or" problem is a function that has two inputs (0,1) and
which returns a true value (1) if both inputs are the same. Otherwise
the result is False (0). This problem looks very trival. However, the
complexity comes from the fact that it is non-linear. The four solutions
cannot be linearly separated. Linear classifiers and one-layer neural
network struggle to solve this problem.

## A Three-Layer-Network

Time for something new...

I created a simple fully-connected neural network. It conists of an input layer
and two hidden layers. How does it look like if we map it out? Do you really
wanna see it? I cannot really draw but I try my best.

Chart

Well, I told you that I cannot draw but I hope you still see it. First, it has
an input layer with two features. We have for each observation two inputs (0,1).
After the input layer follows the first hidden layer. In my attempt to
draw it, I drew 3 neurons (or nodes) in the first hidden layer. This layer and
the network are fully connected. That means all neurons, input features or output
values are connected with each other from one layer to another.

The first neuron gets input from the input feature 1 multiplied by the strength
of the connection weight 1. It also gets input from feature 2 mutliplied by weight 2.
Before the sum of its connection with the input features is passed on to
the next layer, the second hidden layer, the neuron processes the information.
It does this with the help of an activation function. The activation function is
one of the secret ingredients of a neural network because it translates linear
relationships to non-linear activations.

In order to achieve this, there are a number of commonly used activation functions.
A famous one which is often used for simple problems is the sigmoid function. The
sigmoid function takes a value and translates it to an output between 0 and 1.

This is the formula for the function. The function graph looks like a S.
Therefore, the name sigmoid which is derived from the Greek letter "s".
The derivative of the sigmoid function can be written in two ways and both
have the same results. The difference is whether x is the input or
whether the sigmoid function is used to calculate the derivative. This statement
took me one full day to figure it out... but it's like this. In coding scripts,
you may see more often the s*(1-s) formula for the sigmoid derivative as it
is a bit more intuitive and easier to process.

$$s = 1/(1+e^(-x))$$

We can easy plot the functions with python and the help of numpy.
This is the code:

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid (x):
    return (1/(1+np.exp(-x)))

def sigmoid_prime (x):
    return (np.exp(-x)/((1+np.exp(-x))**2))

X = np.linspace(-6,6,100)
y_sig = sigmoid(X)
y_sig_prim = sigmoid_prime(X)

plt.scatter (X,y_sig,linewidths=0.5, color="red", alpha=0.3, label="Sigmoid function")
plt.plot (X,y_sig_prim, linewidth=3, label="Derivatives of Sigmoid")
plt.legend()
plt.savefig("sigmoid.jpg")
```

You can see the charts below. The red line is the sigmoid function. The blue line
its derivative. We can see that the limit of the sigmoid function is 0 and 1
and both values are only reached in infinity. This means for the derivative
that it heads towards 0 for very big and very small input values. On the
other hand, the change of the sigmoid function, the derivative is the
largest in the middle.

<img src="{{ site.url }}{{ site.baseurl }}/images/FIRSTNN/sigmoid.jpg"
alt="Sigmoid function and its derivative">

The sigmoid function is just one example of an activation function. There
are many more such as TanH, Binary Step, and Rectified Linear Unit (ReLU).
For this example, sigmoid function is just fine.

The state-of-the-art libaries for deep learning and artifical neural networks
(ANN) are TensorFlow, Theano, Keras and PyTorch. These are all open-source
libraries which allow builiding neural networks in a few lines of code or
which gives you the flexiblity of building sophisticated and deep networks
with a less complicated python script.

However, in order to really control the architecture and to get the best
results, I think, it is crucial to understand the most aspects of neural
networks. Therefore, I decided first to build one from scratch without
the help of these libraries. I only used numpy for quick matrix and vector
calculations.

I used object-orientated programming and built the ANN as a
python class. Furthermore, I split the basic ANN script from the
execution script. This allows me embedding the execution in loops without
modifying my original ANN script. I just needed to import my ANN script in
my execution file which I call main.py.





---
title: "My first Neural Network"
date: 2018-06-04
tags: [machine learning, neural network]
header:
    image: "images/FIRSTNN/starry_night.jpg"
excerpt: "My first Neural Network. Simple and easy."
---


## My first Neural Network

This is a huge and intimidating topic. It sounds complicated, mysterious,
and impossible to master if you do not have a PhD in Maths, Computer Science,
and Artifical Intelligence. But... it is incredibly exciting. I believe,
as long as you are excited about something you can master it. At the end,
90% of the problems in the world, need just one basic ingredient.
Common sense.

Thinking about Neural Network is for me pure excitement and the more I
explore it, the more I see the power of it. Imagine, how our world will
look like once machines can see, move, and think as individual units and
as part of our society. I am fully certain that this will come and exploring
Neural Networks with today's data availability and computer processing power
is a first step on a journey towards this future.

As the famous Chinese philosopher and founder of the Taoism Lao Zi (老子) said:

## "千里之行始于足下"

("A Journey of a thousand miles begins with a single step")

### The XOR problem

One of the reasons why Neural Networks are so powerful is that they are
able to solve non-linear problems. A famous non-linear problem in Machine
Learning is the XOR problem.

Mapping out all inputs and outputs would look like this:

<ul>
    <li>Input A: 0   Input B: 0   Output: 1</li>
    <li>Input A: 0   Input B: 1   Output: 0</li>
    <li>Input A: 1   Input B: 0   Output: 0</li>
    <li>Input A: 1   Input B: 1   Output: 1</li>
</ul>


Do you see the pattern?

The "exclusive or" problem is a function that has two inputs (0,1) and
which returns a true value (1) if both inputs are the same. Otherwise
the result is False (0). This problem looks very trival. However, the
complexity comes from the fact that it is non-linear. The four solutions
cannot be linearly separated. Linear classifiers and one-layer neural
network struggle to solve this problem.

## A Three-Layer-Network

Time for something new...

I created a simple fully-connected neural network. It conists of an input layer
and two hidden layers. How does it look like if we map it out? Do you really
wanna see it? I cannot really draw but I try my best.

<img src="{{ site.url }}{{ site.baseurl }}/images/FIRSTNN/ANN structure.jpg"
alt="ANN Three Layer Net">

Well, I told you that I cannot draw but I hope you still see it. First, it has
an input layer with two features. We have for each observation two inputs (0,1).
After the input layer follows the first hidden layer. In my attempt to
draw it, I drew 3 neurons (or nodes) in the first hidden layer. This layer and
the network are fully connected. That means all neurons, input features or output
values are connected with each other from one layer to another.

The first neuron gets input from the input feature 1 multiplied by the strength
of the connection weight 1. It also gets input from feature 2 mutliplied by weight 2.
Before the sum of its connections with the input features is passed on to
the next layer, to the second hidden layer, the neurons in the first layer process
the information. It does this with the help of an activation function. The activation
function is one of the secret ingredients of a neural network because it
translates linear relationships to non-linear activations.

In order to achieve this, there are a number of commonly used activation functions.
A famous one which is often used for simple problems is the sigmoid function. The
sigmoid function takes a value and translates it to an output between 0 and 1.

This is the formula for the sigmoid function and its derivative:

<img src="{{ site.url }}{{ site.baseurl }}/images/FIRSTNN/sigmoid functions.jpg"
alt="Sigmoid function and derivatives">

The derivative of the sigmoid function can be written in two ways and both
have the same results. The difference is whether x is the input of the derivative or
whether the sigmoid function is used to calculate it. This understanding took
me one full day to figure it out... haha... but it's like this. In coding
scripts, you may see more often the f(x)*(1-f(x)) formula for the sigmoid derivative as it
is a bit more intuitive and easier to process.

We can easy plot the functions with python and the help of numpy.
This is the code:

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid (x):
    return (1/(1+np.exp(-x)))

def sigmoid_prime (x):
    return (np.exp(-x)/((1+np.exp(-x))**2))

X = np.linspace(-6,6,100)
y_sig = sigmoid(X)
y_sig_prim = sigmoid_prime(X)

plt.scatter (X,y_sig,linewidths=0.5, color="red", alpha=0.3, label="Sigmoid function")
plt.plot (X,y_sig_prim, linewidth=3, label="Derivatives of Sigmoid")
plt.legend()
plt.savefig("sigmoid.jpg")
```

I posted the graph below. The red line is the sigmoid function. The blue line
its derivative. The function graph looks like a S. Therefore, the name sigmoid which is
derived from the Greek letter "s".

We can see that the limit of the sigmoid function is 0 and 1
and both values are only reached in infinity. This means for the derivative
that very big and very small input values lead to a derivative close to 0.
On the other hand, the change of the sigmoid function, the derivative is the
largest in the middle.

<img src="{{ site.url }}{{ site.baseurl }}/images/FIRSTNN/sigmoid.jpg"
alt="Sigmoid function and its derivative">

The sigmoid function is just one example of an activation function. There
are many more such as TanH, Binary Step, and Rectified Linear Unit (ReLU).
For this example, sigmoid function is just fine.

### Ok, but how do you code this network?

The state-of-the-art libaries for deep learning and artifical neural networks
(ANN) are TensorFlow, Theano, Keras and PyTorch. These are all open-source
libraries which allow builiding neural networks in a few lines of code or
which gives you the flexiblity of building sophisticated and deep networks
with a less complicated python script.

However, in order to really control the architecture and to get the best
results, I think, it is crucial to understand the most importatnt aspects
of neural networks. Therefore, I decided first to build one from scratch without
the help of these libraries. I only used numpy for quick matrix and vector
calculations.

I applied object-orientated programming and built the ANN as a
python class. Furthermore, I split the basic ANN script from the
execution script. This allowed me wrapping the execution in loops without
modifying my original ANN script. I just needed to import my ANN script in
my execution file. I called the execution file "main.py" and the ANN script is
in a file called "ann.py".

Let me show you first the script for the basic neural network.

```python

import numpy as np

#Creating the Neural Network as Python Class
class ThreeLayerNet(object):

#Defining the relevant variables and external inputs.
    def __init__(self, X, y, neurons_l2=3, neurons_l3=3, weight_init="uniform", epoch=100):
        self.X = X
        self.y = y
        self.neurons_l2 = neurons_l2
        self.neurons_l3 = neurons_l3
        self.weight_init = weight_init
        self.epoch = epoch
        self.variable_list = ["self.X", "self.y",
                              "self.W1", "self.z2", "self.a2",
                              "self.W2", "self.z3", "self.a3",
                              "self.W3", "self.z4", "self.y_est",
                              "self.error", "self.cost", "self.cost_deriv",
                              "self.delta4", "self.dCdW3",
                              "self.delta3", "self.dCdW2",
                              "self.delta2", "self.dCdW1",
                              ]
        #Three ways to initiate the network weights.
        #Choice 1: Random with normal distributuon.
        if self.weight_init == "normal":
            self.W1 = np.random.randn(self.X.shape[1],self.neurons_l2)
            self.W2 = np.random.randn(self.neurons_l2, self.neurons_l3)
            self.W3 = np.random.randn(self.neurons_l3, self.y.shape[1])

        #Choice 2: Random with uniform distribution.
        elif self.weight_init == "uniform":
            self.W1 = np.random.uniform(0,1,self.X.shape[1]*self.neurons_l2).\
                reshape((self.X.shape[1],self.neurons_l2))
            self.W2 = np.random.uniform(0,1,self.neurons_l2*self.neurons_l3).\
                reshape((self.neurons_l2,self.neurons_l3))
            self.W3 = np.random.uniform(0,1,self.neurons_l3*self.y.shape[1]).\
                reshape((self.neurons_l3,self.y.shape[1]))

        #Choice 3: All weights are initially set at 1.
        else:
            self.W1 = np.ones((self.X.shape[1],self.neurons_l2))
            self.W2 = np.ones((self.neurons_l2,self.neurons_l3))
            self.W3 = np.ones((self.neurons_l3,self.y.shape[1]))

    #Sigmoid function
    def sigmoid (self, z):
        return (1/(1+np.exp(-z)))

    #Derivative of Sigmoid function.
    def sigmoid_deriv (self, z):
        return (np.exp(-z)/(np.power((1+np.exp(-z)),2)))

    '''
    This function manages the so-called forward propagation. Calculating the output
    step by step.

    The dot product of the first layer (input layer X) and weights matrix W1 results
    in the second layer (z2). Then applying the activation function sigmoid to z2
    and receiving the first activation layer a2. The next step is connecting a2 with
    the second hidden layer by calculating the dot product of a2 and weight matrix W2.
    This gives z3 and passing it through the sigmoid function leads to a3. Dot product
    of a3 with W3 is z4 and applying the sigmoid function on the last layer gives us
    the estimated output y_est.
    '''

    def forward_propagation(self):
        self.z2 = np.dot(self.X,self.W1)
        self.a2 = self.sigmoid(self.z2)

        self.z3 = np.dot(self.a2,self.W2)
        self.a3 = self.sigmoid(self.z3)

        self.z4 = np.dot(self.a3,self.W3)
        self.y_est = self.sigmoid(self.z4)

    '''
    This function calculates the error, the cost function, the difference between
    the estimated output y_est and the actual value y. It also calculates the
    derivative as a first input for backpropagation.
    '''

    def loss_calculation(self):
        self.error = self.y - self.y_est
        self.cost = 0.5*np.sum((np.power(self.error,2)))
        self.cost_deriv = self.y_est - self.y

    '''
    This is the famous backpropagation. The major technique that allows an ANN
    to learn and improve its results. Basically, what it does is it takes the prediction
    error and tries to see which parts of the network, which weights are most important
    in creating an error.

    Mathematically, we take the partial derivatives layer by layer
    and end up with the calculated sensitivity in respect to the weight matrices.

    Delta is the sensitivity of the layer cost and dCdW puts this sensitivity in respect
    to a specific weight matrix. In other words, how much does the error (or cost C)
    change if we change the weights W.
    '''

    def back_propagation(self):
        self.delta4 = np.multiply(self.cost_deriv,self.sigmoid_deriv(self.z4))
        self.dCdW3 = np.dot(self.a3.T,self.delta4)

        self.delta3 = np.multiply(np.dot(self.delta4,self.W3.T),self.sigmoid_deriv(self.z3))
        self.dCdW2 = np.dot(self.a2.T,self.delta3)

        self.delta2 = np.multiply(np.dot(self.delta3,self.W2.T),self.sigmoid_deriv(self.z2))
        self.dCdW1 = np.dot(self.X.T,self.delta2)

    '''
    This function trains the network by applying for each epoch the forward propagation,
    the loss_calculation and the back_propagation. After that it updates the weight
    by the sensitivities and recalculates everything for the next round, the next epoch.
    '''

    def train_NN (self):
        for i in range(0,self.epoch+1,1):
            self.forward_propagation()
            self.loss_calculation()
            self.back_propagation()
            self.W1 -= self.dCdW1
            self.W2 -= self.dCdW2
            self.W3 -= self.dCdW3

    #Printing the shape of all matrices
    def print_shape(self):
        for var in self.variable_list:
            print ("\n {} \n {}".format(var, eval(var).shape))

    #Printing the values of all matrices
    def print_values (self):
        for var in self.variable_list:
            print ("\n {} \n {}".format(var, eval(var)))

```

This includes quite a lot of concepts. So, let's break it down step by
step.

First, I created a python class and name it "ThreeLayerNet". By initiating
the class, I also defined the class relevant variables. I also put all
the variables in a list. That made it easier for me to iterate through
all the variables when I wanted to print values and shapes of each layer.

One point often raised by critical voices is that neural networks are a
"black box". For me it was important that I understand what happened at
each layer and in each neuron. The functions for printing the shape and values
are at the lower part of the script.

I also decided to see what happened if the weight initiation follows different
principles. Every ANN needs to start somewhere. At the very beginning, at the
time when the weights are not trained, the weights need to be set somewhere.
There are couple of ways to do that and research showed that weight initiation
is a critical part of the training success of neural networks. Simple networks can
be initiated weights that are chosen randomly. I built in





